{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5Ri0JoS1yFy/J/4HbeFST",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavlata/LLM-experiments/blob/main/Fewshot_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WezqZG2foPzf"
      },
      "outputs": [],
      "source": [
        "! pip install -q langchain transformers sentence_transformers\n",
        "!pip install chromadb\n",
        "!pip install unstructured\n",
        "!pip install accelerate\n",
        "!pip install -i https://test.pypi.org/simple/ bitsandbytes\n",
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glA-axu3vaMt",
        "outputId": "86643c46-9b25-4e21-a6f4-027a9bed243e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (8.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this to load your documents. I have used Contracts from the CUAD Dataset - https://www.atticusprojectai.org/cuad\n",
        "# Since we want to extract important details from a particular contract, we are doing single document loading and indexing\n",
        "from langchain.text_splitter import CharacterTextSplitter, TextSplitter #, NLTKTextSplitter\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "loader = DirectoryLoader('/content/sample_data/', glob=\"**/*.pdf\")\n",
        "documents = loader.load()\n",
        "documents"
      ],
      "metadata": {
        "id": "mrq9wyRdoVzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yvk4gnylgSa",
        "outputId": "7c8d424c-1e82-4a4e-ff4a-accd75f7d0f4"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The document is split into multiple chunks using the splitter below. I have kept overlap so that text continuity is preserved \n",
        "# between the chunks.\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(        \n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap  = 200, #striding over the text\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJNTNP9XoYR8",
        "outputId": "80142133-e810-42b8-85bb-2daf31187156"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 1405, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1298, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 850, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 801, which is longer than the specified 800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for indexing a document using Chroma using HuggingFaceEmbeddings.\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from chromadb.config import Settings\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "collection_name=\"long-docs\"\n",
        "persist_directory=\"/content/chromadb/\"\n",
        "\n",
        "client_settings = Settings(chroma_db_impl=\"duckdb+parquet\",persist_directory=persist_directory, anonymized_telemetry=False)\n",
        "vectorstore = Chroma(collection_name=collection_name,embedding_function=embeddings,client_settings=client_settings,persist_directory=persist_directory)\n",
        "\n",
        "vectorstore.add_documents(documents=texts, embedding=embeddings)\n",
        "vectorstore.persist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6B1L8Cupgyx",
        "outputId": "2cabb04d-83c2-438e-ce00-ab685caeba06"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: /content/chromadb/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This part is for loading the LLM Model from HuggingFace. This downloads the code in memory.\n",
        "# For running model on client data, downloading the model on your local server would be highly recommended instead of \n",
        "# using Hugging Face hub\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id = 'google/flan-t5-large'# go for a smaller model if you dont have the VRAM\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\"text2text-generation\",model=model, tokenizer=tokenizer, max_length=150,\n",
        "                model_kwargs={\"temperature\":0}) #text2text-generation , text-generation\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "s7DGDpWPpd1v"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to provide few shot examples to your prompt use the below\n",
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "examples = [\n",
        "            # { \n",
        "            #       \"question\": \"Which are the parties in this agreement ?\",\n",
        "            #     \"summaries\" : \"THIS ENDORSEMENT AGREEMENT (the Agreement) is dated as of this \\\n",
        "            #      ____day of ____________, 2012, but made effective as of February 20, 2012 (Effective Date) between\\\n",
        "            #       Healthcare Distribution Specialists LLC (HDS), a Delaware corporation, and Paul Silas (Celebrity),\\\n",
        "            #        an individual.\"\n",
        "            # },\n",
        "#             {\n",
        "#                     \"question\": \"What is the contract date?\",\n",
        "#                    \"summaries\":\n",
        "#                     \"\"\" THIS AGREEMENT is made and entered into this 1st day of June, 2017 by and between Prudential Bank (hereinafter referred to as the\n",
        "# \"Employer\"), located in Philadelphia, Pennsylvania and Jeffrey Hanuscin, (hereinafter referred to as the \"Employee\"), residing at 2406 Sanibel Circle,\n",
        "# Palmyra, NJ 08065 \"\"\"},\n",
        "            {\n",
        "                  \"question\": \"Which law governs this contract?\",\n",
        "                  \"summaries\" : \"\"\" s Agreement shall be governed by and construed in accordance with the\n",
        "                   laws of the State \"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"question\" : \"What is the address of the employer?\",\n",
        "                \"summaries\" : \"\"\" THIS ENDORSEMENT AGREEMENT (herein the “Agreement”) is effective on this 21st day of February 2011, by and between Golfers \n",
        "Incorporated, a Delaware Corporation, having a mailing address of 1021 N. Sepulveda Blvd., Suite G, Manhattan Beach, CA 90266 (hereinafter \n",
        "referred to as “Company”) and Andy North, having a mailing address of 1624 S. High Point Road, Madison, WI 53719 (hereinafter referred to as \n",
        "“North”).\n",
        "\"\"\"\n",
        "            }\n",
        "]\n",
        "example_template = \"\"\"\n",
        "User: {question}\n",
        "AI: {summaries}\n",
        "\"\"\"\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"summaries\"],\n",
        "    template=example_template\n",
        ")\n",
        "prefix = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references  \n",
        "If you don't know the answer, just say that you don't know.  Here are some\n",
        "examples: \n",
        "\"\"\"\n",
        "suffix = \"\"\"\n",
        "QUESTION: {question}\n",
        "Context : {summaries}\n",
        "Answer :\n",
        "=========\n",
        "\"\"\"\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix = suffix,\n",
        "    input_variables=[\"question\",\"summaries\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "yEpBsRbUoY6I"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "chain = load_qa_with_sources_chain(llm=local_llm, chain_type=\"stuff\",prompt=few_shot_prompt_template)\n",
        "query = \"Where are the parties located ?  \"\n",
        "docs = vectorstore.similarity_search(query,k=1)\n",
        "result = chain({\"input_documents\": docs, \"question\": query}, \n",
        "               return_only_outputs=True)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvzLEYVKqkG-",
        "outputId": "dc8ee714-38b6-4761-ba41-d63b13d25bfd"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'output_text': 'Manhattan Beach, CA 90266'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The below is Zero-Shot prompt\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "template = \"\"\"Given the following extracted parts of a long document and a question, \n",
        "create a final answer . \n",
        "If you don't know the answer, just say that you don't know. \n",
        "\n",
        "QUESTION: {question}\n",
        "=========\n",
        "{summaries}\n",
        "=========\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "# create a prompt template\n",
        "PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
        "# query \n",
        "chain1 = load_qa_with_sources_chain(llm=local_llm, chain_type=\"stuff\",prompt=PROMPT)\n",
        "query1 = \"Where are the parties located ?  \"\n",
        "docs1 = vectorstore.similarity_search(query,k=1)\n",
        "result1 = chain1({\"input_documents\": docs1, \"question\": query1}, \n",
        "               return_only_outputs=True)\n",
        "print(result1)"
      ],
      "metadata": {
        "id": "RzHpylB5qvm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f032d2-e209-4054-e13b-c68003383c16"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'output_text': 'Company and North'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lRDaTnWsKDtA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}